{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorial\n",
    "\n",
    "Deep Learning Tutorial based on https://spandan-madan.github.io/DeepLearningProject/.\n",
    "\n",
    "Credits: [Spandan Madan](http://people.csail.mit.edu/smadan/web/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "* In this section we import required packages.\n",
    "* We can install these packages using the notebook itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inline figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Python packages from the notebook\n",
    "import sys\n",
    "#!conda install -c conda-forge --yes --prefix {sys.prefix} urllib2 # did not work cause of Python 2\n",
    "# Change to python 3 by using urllib instead\n",
    "#!{sys.executable} -m pip install urllib2 # did not work cause of Python 2\n",
    "#!conda install -c conda-forge --yes --prefix {sys.prefix} wget # did not work\n",
    "#!{sys.executable} -m pip install wget # worked\n",
    "#!{sys.executable} -m pip install imdb # did not work\n",
    "#!{sys.executable} -m pip install IMDbPY # worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [Jake Van Der Plas](http://jakevdp.github.io) blog post on [Installing Python packages on a Jupyter Notebook](http://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web access packages\n",
    "import urllib\n",
    "import requests\n",
    "import wget\n",
    "\n",
    "# Utilities packages\n",
    "import itertools\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Movies database API packages\n",
    "import imdb \n",
    "import tmdbsimple as tmdb\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random as rd\n",
    "\n",
    "# Figure and style packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Object serialization package\n",
    "import pickle\n",
    "\n",
    "# Custom utility functions \n",
    "from dltutorial.utils import get_movie_id_tmdb\n",
    "from dltutorial.utils import get_movie_info_tmdb\n",
    "from dltutorial.utils import get_movie_genres_tmdb\n",
    "from dltutorial.utils import get_api_key_tmdb\n",
    "from dltutorial.utils import get_movie_poster_tmdb\n",
    "\n",
    "from dltutorial.utils import get_movie_info_imdb\n",
    "from dltutorial.utils import get_movie_genres_imdb\n",
    "\n",
    "# Importing relevant scikit-learn package\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-darkgrid')\n",
    "sns.set(rc={'figure.figsize':(9, 6)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "* Here we instanciate a logging object to record our logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"./dltutorial/logs/dev_logs.txt\",\n",
    "                    level=logging.INFO,\n",
    "                    format=' %(asctime)s - %(funcName)s -'\n",
    "                    '%(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "* You may want to create a private Python module containing only an ``__init__.py`` file that consists in a string variable ``API_KEY`` that is your private API key from [TMDB](https://www.themoviedb.org/). Else you can instanciate the ``api_key`` directly in your notebook but do not share it!\n",
    "\n",
    "* Custom utility functions have been implemented in ``dltutorial.utils``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We create a folder where we store the scrapped movie posters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Setting posters storage folder...')\n",
    "\n",
    "# Create a posters folder inside data folder\n",
    "posters_path = './dltutorial/data/posters/'\n",
    "\n",
    "        \n",
    "if not os.path.exists(posters_path):\n",
    "    logging.debug('%s does not exists. Creating relevant folders...' %\n",
    "                 posters_path)\n",
    "    # Recursive folder creation\n",
    "    os.makedirs(posters_path)\n",
    "else:\n",
    "    logging.debug('%s already exists...' % posters_path)\n",
    "    print('%s already exists...' % posters_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples\n",
    "\n",
    "### TMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Setting TMDB API key...')\n",
    "if 'private' in os.listdir('./dltutorial'): #\n",
    "    logging.debug('Private folder exists...')\n",
    "    from dltutorial import private\n",
    "    api_key = private.API_KEY\n",
    "else:\n",
    "    logging.debug('No private folder found...')\n",
    "    print('There is no private folder.'\n",
    "          'API key will remain blank if you do not set it.')\n",
    "    api_key = '' # put your own API key but do not share it\n",
    "# Set the TMDB API key\n",
    "tmdb.API_KEY = api_key \n",
    "logging.debug('TMDB API key is set to %s...' % api_key)\n",
    "\n",
    "# Instanciate a search object from TMDB\n",
    "search_tmdb = tmdb.Search()\n",
    "\n",
    "# Some examples\n",
    "movie_name = \"The Matrix\"\n",
    "\n",
    "movie_id = get_movie_id_tmdb(movie_name=movie_name, search_tmdb=search_tmdb)\n",
    "print(\"%s has id %s\\n\" % (movie_name, movie_id))\n",
    "movie_info = get_movie_info_tmdb(movie_name=movie_name, search_tmdb=search_tmdb)\n",
    "print(\"%s has these info categories:\\n %s\\n\" % (movie_name, movie_info.keys()))\n",
    "movie_genres = get_movie_genres_tmdb(movie_name=movie_name, search_tmdb=search_tmdb)\n",
    "print(\"%s belongs to these genres:\\n %s\\n\" % (movie_name, movie_genres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_name = \"The Matrix\"\n",
    "search_imdb = imdb.IMDb()\n",
    "\n",
    "movie_genres = get_movie_genres_imdb(movie_name=movie_name, search_imdb=search_imdb)\n",
    "print(\"%s movie belongs to genres:\\n %s \\n\" % (movie_name, movie_genres))\n",
    "movie_info = get_movie_info_imdb(movie_name=movie_name, search_imdb=search_imdb)\n",
    "print(\"%s movie first 10 info categories:\\n %s \\n\" % (movie_name, movie_info.keys()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 20 from TMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_dict_file = './dltutorial/data/genres_dict.pckl'\n",
    "\n",
    "# Top 20 popular films\n",
    "top_movies = tmdb.Movies().popular()['results']\n",
    "# Create a TMDB genre object\n",
    "genres = tmdb.Genres()\n",
    "# Returns the genres list (ID/name)\n",
    "genres_list = genres.movie_list()['genres']\n",
    "\n",
    "# if genres dictionary does not exists\n",
    "if not os.path.exists(genres_dict_file):\n",
    "    # Create a genres dictionary\n",
    "    genres_dict = dict()\n",
    "    for genre in genres_list:\n",
    "        genres_dict[genre['id']] = genre['name']\n",
    "    logging.debug(\"Writing genres dict on hard disk...\")\n",
    "    with open(genres_dict_file, 'wb') as f:\n",
    "        pickle.dump(genres_dict, f)\n",
    "    logging.debug(\"Done saving genres dict in %s...\" %\n",
    "                 genres_dict_file)\n",
    "# Else load existing one\n",
    "else:\n",
    "    logging.debug(\"Loading genres dict from hard disk...\")\n",
    "    with open(genres_dict_file, 'rb') as f:\n",
    "        genres_dict = pickle.load(f)\n",
    "    logging.debug(\"Done loading genres dict from %s...\" %\n",
    "                 genres_dict_file)\n",
    "\n",
    "\n",
    "# Print the genres of top 5 movies\n",
    "for movie in top_movies[:5]:\n",
    "    genre_ids = movie['genre_ids']\n",
    "    genre_names = list()\n",
    "    for genre_id in genre_ids: \n",
    "        genre_names.append(genres_dict[genre_id])\n",
    "    print(\"%s\\n%s\\n\" % (movie['title'], genre_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 1000 movies from TMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_list_file = './dltutorial/data/tmdb_movie_list.pckl'\n",
    "\n",
    "if not os.path.exists(movie_list_file):\n",
    "    # Instantiate TMDB movies object\n",
    "    all_movies = tmdb.Movies()\n",
    "    # Instantiate empty list of movies to populate\n",
    "    top1000_movies = list()\n",
    "    # Number of pages to download\n",
    "    n_pages = 50\n",
    "    logging.debug(\"Downloading movie list from top %s pages...\" % n_pages)\n",
    "    for i in range(1, n_pages + 1):\n",
    "        if i%10 == 0:\n",
    "            msg = \"%s out of %s movie pages downloaded (%s%%)...\" % (i, n_pages, round(100 * i / n_pages, 1))\n",
    "            print(msg)\n",
    "            logging.debug(msg)\n",
    "            time.sleep(5)\n",
    "        current_page_movies = all_movies.popular(page=i)['results']\n",
    "        top1000_movies.extend(current_page_movies)\n",
    "    # Write binary in data folder in pickle format\n",
    "    logging.debug(\"Writing %s movie results on disk...\" % (len(top1000_movies)))\n",
    "    with open(file=movie_list_file, mode='wb') as f:\n",
    "        pickle.dump(top1000_movies, f)\n",
    "    logging.debug(\"Done writing...\")\n",
    "else:\n",
    "    # Open binary pickle file to load movie list variable\n",
    "    logging.debug(\"Loading movies list from disk...\")\n",
    "    with open(file=movie_list_file, mode='rb') as f:\n",
    "        top1000_movies = pickle.load(f)\n",
    "    logging.debug(\"Done loading %s movies\" % len(top1000_movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with unbalanced dataset\n",
    "\n",
    "Handling the multi-label component. Co-occurrence sheds some information that the dataset has imbalances.\n",
    "\n",
    "* We want to investigate pairwise genres correlations.\n",
    "    * First we build every possible pair from the movie genres available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2pairs(input_list):\n",
    "    \"\"\"\n",
    "    Generating all possible pairs of movies\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    - input_list: list\n",
    "        generic list used to generate pairs\n",
    "        \n",
    "    Returns\n",
    "    -----------\n",
    "    - pairs: list\n",
    "        list of pairs (tuples)\n",
    "    \"\"\"\n",
    "    # Build all pairs of length 2 from input_list\n",
    "    pairs = list(itertools.combinations(input_list, 2))\n",
    "    # Add duplicate pairs\n",
    "    pairs.extend([(elt, elt) for elt in input_list])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Debugging part: dropping missing genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all genre pairs from top 1000 movies\n",
    "genre_pairs = list()\n",
    "for movie in top1000_movies:\n",
    "    genre_pairs.extend(list2pairs(movie['genre_ids']))\n",
    "    \n",
    "# We keep only unique pairs\n",
    "unique_genres = np.unique(genre_pairs)\n",
    "# We instantiate a correlation matrix\n",
    "corr_matrix = np.zeros((len(unique_genres), len(unique_genres)))\n",
    "for pair in genre_pairs:\n",
    "    corr_matrix[np.argwhere(unique_genres == pair[0]), np.argwhere(unique_genres == pair[1])] += 1\n",
    "    # If not duplicate pairs, affect symmetrical coefficient\n",
    "    if pair[1] != pair[0]:\n",
    "        corr_matrix[np.argwhere(unique_genres == pair[1]), np.argwhere(unique_genres == pair[0])] += 1\n",
    "    \n",
    "# Handling missing genres\n",
    "#try:\n",
    "#    assert len(genres_dict.keys()) == corr_matrix.shape[0]\n",
    "#    assert len(genres_dict.keys()) == corr_matrix.shape[1]\n",
    "#except:\n",
    "#    print(\"Number of genres from TMDB %d\" % len(genres_dict.keys()))\n",
    "#    print(\"Correlation matrix shape: \", corr_matrix.shape)\n",
    "#    missing_genres = [genre for genre in list(genres_dict.keys()) if genre not in unique_genres]\n",
    "#    for genre in missing_genres:\n",
    "#        print(\"Removing genre %s from genres dictionary...\" % genres_dict[genre])\n",
    "#        genres_dict.pop(genre, None)\n",
    "#    assert len(genres_dict.keys()) == corr_matrix.shape[0]\n",
    "#    assert len(genres_dict.keys()) == corr_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation matrix visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return genres names\n",
    "annotations = [genres_dict[genre_id] for genre_id in unique_genres]\n",
    "# Display correlation matrix\n",
    "sns.heatmap(corr_matrix, xticklabels=annotations, yticklabels=annotations);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Diagonal terms correspond to self-pairs, i.e. the number of times a genre co-occur. That is nothing else than the total times said genre occurred. *We observe that Drama seems to be the most common genre*.\n",
    "\n",
    "* Additionally we notice that Action often occurs alongside Adventure, Thriller and Mystery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biclustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralCoclustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = SpectralCoclustering(n_clusters=5, random_state=42)\n",
    "# Fit model\n",
    "model.fit(corr_matrix)\n",
    "\n",
    "# Rearrange data based on bi-clusters\n",
    "fit_data = corr_matrix[np.argsort(model.row_labels_)]\n",
    "fit_data = fit_data[:, np.argsort(model.column_labels_)]\n",
    "\n",
    "# Build rearranged annotations\n",
    "sorted_annotations = [genres_dict[unique_genres[idx]] for idx in np.argsort(model.row_labels_)]\n",
    "\n",
    "# Display biclustering aftermath\n",
    "sns.heatmap(fit_data, xticklabels=sorted_annotations, yticklabels=sorted_annotations)\n",
    "plt.title(\"Post Bi-Clustering. Rearranged to display biclusters\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We see genres are clustered into groups. For instance: *Thriller, Action, Horror and Crime* belong to 1 cluster while *Adventure, Fantasy, Animation, Comedy and Family* belong to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping TMDB movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_movies_list_file = './dltutorial/data/tmdb_genre_movies_list.pckl'\n",
    "\n",
    "if not os.path.exists(genre_movies_list_file):\n",
    "    # Instantiate empty movies list\n",
    "    genre_movies = list()\n",
    "    # Set base year to prevent duplicates\n",
    "    # We decrement the base_year for every genre,\n",
    "    # increasing our chances to encounter duplicate movies\n",
    "    base_year = 2017\n",
    "    # Set number of pages to download per genre\n",
    "    n_pages_per_genre = 5\n",
    "    print(\"Start downloading...\")\n",
    "    logging.debug(\"Starting downloading movies from TMDB.\"\n",
    "                 \"Each genre will download %d movies...\" %\n",
    "                  (20 * n_pages_per_genre))\n",
    "    for genre_id in unique_genres:\n",
    "        logging.debug(\"Downloading movies from genre %s...\" %\n",
    "                     genres_dict[genre_id])\n",
    "        print(\"Downloading movies from genre %s...\" %\n",
    "                     genres_dict[genre_id])\n",
    "        # Decrement base_year. Described above\n",
    "        base_year -= 1\n",
    "        for page_id in range(1, n_pages_per_genre + 1):\n",
    "            # Let TMDB server have some rest\n",
    "            time.sleep(0.5)\n",
    "            # Setting URL using parameters\n",
    "            url = 'https://api.themoviedb.org/3/discover/movie?api_key=' + api_key\n",
    "            url += '&language=en-US&sort_by=popularity.desc&year=' + str(base_year)\n",
    "            url += '&with_genres=' + str(genre_id) + '&page=' + str(page_id)\n",
    "            # Retrieve results\n",
    "            data = urllib.request.urlopen(url).read().decode('utf8')\n",
    "            # Append results to genre_movies\n",
    "            genre_movies.extend(json.loads(data)[\"results\"])\n",
    "    print(\"Done downloading...\")\n",
    "\n",
    "    # Write binary in data folder in pickle format\n",
    "    logging.debug(\"Writing %s movie results on disk...\" % (len(genre_movies)))\n",
    "    with open(file=genre_movies_list_file, mode='wb') as f:\n",
    "        pickle.dump(genre_movies, f)\n",
    "    logging.debug(\"Done writing...\")\n",
    "else:\n",
    "    # Open binary pickle file to load movie list variable\n",
    "    logging.debug(\"Loading movies list from disk...\")\n",
    "    with open(file=genre_movies_list_file, mode='rb') as f:\n",
    "        genre_movies = pickle.load(f)\n",
    "    logging.debug(\"Done loading %s movies...\" % len(genre_movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate empty id list\n",
    "seen_ids = list()\n",
    "# Instantiate empty movie list\n",
    "unique_movies = list()\n",
    "logging.debug(\"Originally we had %s movies...\" % len(genre_movies))\n",
    "print(\"Originally we had %s movies...\" % len(genre_movies))\n",
    "for movie in genre_movies:\n",
    "    movie_id = movie['id']\n",
    "    if movie_id in seen_ids:\n",
    "        continue\n",
    "    else:\n",
    "        seen_ids.append(movie_id)\n",
    "        unique_movies.append(movie)\n",
    "# Assert if we rightfully removed duplicates\n",
    "assert len(unique_movies) == len(np.unique([movie['id'] for movie in genre_movies]))\n",
    "logging.debug(\"After removing duplicates we have %s movies...\"\n",
    "              % len(unique_movies))\n",
    "print(\"Now we have %s movies...\" % len(unique_movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poster and posterless movies\n",
    "\n",
    "* Here we create two lists based on whether the movie has a poster or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poster_movies_list_file = './dltutorial/data/posters/poster_movies_list.pckl'\n",
    "posterless_movies_list_file = './dltutorial/data/posters/posterless_movies_list.pckl'\n",
    "\n",
    "if not os.path.exists(poster_movies_list_file) or not os.path.exists(posterless_movies_list_file):\n",
    "    # Instantiate empty lists\n",
    "    poster_movies = list()\n",
    "    posterless_movies = list()\n",
    "\n",
    "    logging.debug(\"Starting downloading posters for %s movies...\" %\n",
    "                 len(unique_movies))\n",
    "    print(\"Starting downloading posters for %s movies...\" %\n",
    "                 len(unique_movies))\n",
    "    for counter, movie in enumerate(unique_movies):\n",
    "        # Keep track of current progress\n",
    "        if counter % 400 == 0 and counter != 0:\n",
    "            print(\"%d movies downloaded (%s%%)\" %\n",
    "                  (counter, round(100 * counter / len(unique_movies))))\n",
    "            logging.debug(\"%d movies downloaded (%s%%)\" %\n",
    "                  (counter, round(100 * counter / len(unique_movies))))\n",
    "        # Try getting the poster\n",
    "        try:\n",
    "            get_movie_poster_tmdb(movie['title'], search_tmdb=search_tmdb, path=complete_path)\n",
    "            poster_movies.append(movie)\n",
    "        # If too many requests, wait\n",
    "        except:\n",
    "            try:\n",
    "                time.sleep(7)\n",
    "                get_movie_poster_tmdb(movie['title'], search_tmdb=search_tmdb, path=complete_path)\n",
    "                poster_movies.append(movie)\n",
    "            # If poster is missing, movie is posterless\n",
    "            except:\n",
    "                posterless_movies.append(movie)\n",
    "    logging.debug(\"Done downloading. %d with poster. %d posterless\" %\n",
    "                 (len(poster_movies), len(posterless_movies)))\n",
    "    print(\"Done downloading. %d with poster. %d posterless\" %\n",
    "                 (len(poster_movies), len(posterless_movies)))\n",
    "    \n",
    "    # Write binary in data folder in pickle format\n",
    "    logging.debug(\"Writing %s poster movies results on disk...\" % (len(poster_movies)))\n",
    "    with open(file=poster_movies_list_file, mode='wb') as f:\n",
    "        pickle.dump(poster_movies, f)\n",
    "    logging.debug(\"Done writing...\")\n",
    "    logging.debug(\"Writing %s posterless movies results on disk...\" % (len(posterless_movies)))\n",
    "    with open(file=posterless_movies_list_file, mode='wb') as f:\n",
    "        pickle.dump(posterless_movies, f)\n",
    "    logging.debug(\"Done writing...\")\n",
    "    \n",
    "else:\n",
    "    # Open binary pickle file to load movie list variable\n",
    "    logging.debug(\"Loading poster movies list from disk...\")\n",
    "    with open(file=poster_movies_list_file, mode='rb') as f:\n",
    "        poster_movies = pickle.load(f)\n",
    "    logging.debug(\"Done loading %s poster movies...\" % len(poster_movies))\n",
    "    print(\"Done loading %s poster movies...\" % len(poster_movies))\n",
    "    \n",
    "    logging.debug(\"Loading posterless movies list from disk...\")\n",
    "    with open(file=posterless_movies_list_file, mode='rb') as f:\n",
    "        posterless_movies = pickle.load(f)\n",
    "    logging.debug(\"Done loading %s posterless movies...\" % len(posterless_movies))\n",
    "    print(\"Done loading %s posterless movies...\" % len(posterless_movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the dataset\n",
    "\n",
    "* We stick with the original implementation first:\n",
    "  * We will use 2 different kinds of features - text and images.\n",
    "    * text: movie's plot available from TMDB.\n",
    "    * image: movie's poster available from TMDB.\n",
    "  * We will use multi-labels as targets.\n",
    "  \n",
    "  \n",
    "* How to store movies overview in a matrix?\n",
    "\n",
    "*Using a count vectorizer to store words with a *bag of words* representation.*\n",
    "\n",
    "* How to avoid the curse of dimensionality?\n",
    "\n",
    "*Using a TF-IDF to discard words either too frequent or too rare.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_file = './dltutorial/data/targets_text.pckl'\n",
    "inputs_file = './dltutorial/data/inputs_text.pckl'\n",
    "overview_movies_file = './dltutorial/data/overview_movies.pckl'\n",
    "\n",
    "if not os.path.exists(targets_file) or not os.path.exists(inputs_file) or not os.path.exists(overview_movies_file):\n",
    "    # Picking movies with available overview\n",
    "    overview_movies = [movie for movie in poster_movies if len(movie['overview']) > 0]\n",
    "    print(\"%d movies have available overview...\" % len(overview_movies))\n",
    "    # Making an iterable of iterables (genre_ids)\n",
    "    movie_genre_ids = [movie['genre_ids'] for movie in overview_movies]\n",
    "    print(\"Movie genre sample:\\n%s\" % movie_genre_ids[1])\n",
    "    from sklearn.preprocessing import MultiLabelBinarizer\n",
    "    # Turning genre_ids into multi labels\n",
    "    targets = MultiLabelBinarizer().fit_transform(movie_genre_ids)\n",
    "    print(\"Multi-labels shape: %d by %d\" % (targets.shape))\n",
    "    logging.debug(\"Multi-labels shape: %d by %d\" % (targets.shape))\n",
    "    print(\"Genre counts:\\n%s\" % (np.sum(targets, axis=0)))\n",
    "    # Get rid of the punctuation\n",
    "    overviews = [movie['overview'].replace(',', '').replace('.', '') for movie in overview_movies]\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    # min_df exclude words that occur very rarely\n",
    "    # max_df exclude words that occur too often\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=0.005)\n",
    "    inputs = vectorizer.fit_transform(overviews)\n",
    "    print(\"Inputs shape: %d by %d\" % (inputs.shape))\n",
    "    logging.debug(\"Inputs shape: %d by %d\" % (inputs.shape))\n",
    "    \n",
    "    # Writing inputs in pickle format\n",
    "    logging.debug(\"Writing %d inputs on disk...\" % inputs.shape[0])\n",
    "    with open(inputs_file, 'wb') as f:\n",
    "        pickle.dump(inputs, f)\n",
    "    logging.debug(\"Done writing inputs in %s...\" % inputs_file)\n",
    "    # Writing targets in pickle format\n",
    "    logging.debug(\"Writing %d targets on disk...\" % targets.shape[0])\n",
    "    with open(targets_file, 'wb') as f:\n",
    "        pickle.dump(targets, f)\n",
    "    logging.debug(\"Done writing targets in %s...\" % targets_file)\n",
    "    # Writing overview movies in pickle format\n",
    "    logging.debug(\"Writing %d overview movies on disk...\" % len(overview_movies))\n",
    "    with open(overview_movies_file, 'wb') as f:\n",
    "        pickle.dump(overview_movies, f)\n",
    "    logging.debug(\"Done writing overview movies in %s...\" % overview_movies_file)\n",
    "    \n",
    "    \n",
    "else:\n",
    "    # Opening binary pickle to load inputs, targets and genres dictionary variables\n",
    "    # Inputs\n",
    "    logging.debug(\"Loading inputs from disk (%s)...\" % inputs_file)\n",
    "    with open(inputs_file, 'rb') as f:\n",
    "        inputs = pickle.load(f)\n",
    "    logging.debug(\"Done loading %s inputs...\" % inputs.shape[0])\n",
    "    # Targets\n",
    "    logging.debug(\"Loading targets from disk (%s)...\" % targets_file)\n",
    "    with open(targets_file, 'rb') as f:\n",
    "        targets = pickle.load(f)\n",
    "    logging.debug(\"Done loading %s targets...\" % targets.shape[0])\n",
    "    # Overview movies\n",
    "    logging.debug(\"Loading overview movies from disk (%s)...\" % overview_movies_file)\n",
    "    with open(overview_movies_file, 'rb') as f:\n",
    "        overview_movies = pickle.load(f)\n",
    "    logging.debug(\"Done loading %s overview movies...\" % len(overview_movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see here, the data size is really small - around 1k samples.\n",
    "For better results we need to scrap more data but this tutorial's goal is to get familiar with conventional ML and DL techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional ML models\n",
    "\n",
    "Overview:\n",
    "* Implementing two models.\n",
    "* Benchmarking the two models using a performance metric.\n",
    "* Discuss the differences between the two models.\n",
    "\n",
    "A lot of decisions can be made as this point. We can decide to use Non-Bayesian approches, such as:\n",
    "* Generalized Linear Models\n",
    "* Support Vector Machines (SVM)\n",
    "* Shallow Neural Network (only 1 layer)\n",
    "* Random Forest / Decision Tree / Boosting\n",
    "\n",
    "Or use Bayesian approaches:\n",
    "* Naive Bayes\n",
    "* Linear or Quadratic Discriminant Analysis\n",
    "* Bayesian Hierarchical Models\n",
    "\n",
    "For demonstration purposes we will pick one out of both approaches.\n",
    "\n",
    "### Features engineering\n",
    "\n",
    "* We use TF-IDF to assign weight to every word in the bag of words - for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_tfidf_file = './dltutorial/data/inputs_text_tfidf.pckl'\n",
    "\n",
    "if not os.path.exists(inputs_tfidf_file):\n",
    "    logging.debug(\"Using TF-IDF on inputs...\")\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    inputs_tfidf = TfidfTransformer().fit_transform(inputs)\n",
    "    logging.debug(\"TF-IDF done...\")\n",
    "    logging.debug(\"Writing TF-IDF inputs on disk...\")\n",
    "    with open(inputs_tfidf_file, 'wb') as f:\n",
    "        pickle.dump(inputs_tfidf, f)\n",
    "    logging.debug(\"Done writing TF-IDF inputs in %s...\" %\n",
    "                 inputs_tfidf_file)\n",
    "    print(\"TF-IDF inputs shape: (%d, %d)\" % inputs_tfidf.shape)\n",
    "    \n",
    "else:\n",
    "    logging.debug(\"Loading TF-IDF inputs from disk (%s)...\" % \n",
    "                 inputs_tfidf_file)\n",
    "    with open(inputs_tfidf_file, 'rb') as f:\n",
    "        inputs_tfidf = pickle.load(f)\n",
    "    logging.debug(\"Done loading TF-IDF inputs...\")\n",
    "    print(\"TF-IDF inputs shape: (%d, %d)\" % inputs_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into training and testing sets\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(inputs_tfidf, targets,\n",
    "#                                                   test_size=0.2, random_state=42)\n",
    "    \n",
    "X_train = inputs_tfidf[:round(0.8 * inputs_tfidf.shape[0]), :]\n",
    "y_train = targets[:round(0.8 * targets.shape[0]), :]\n",
    "X_test = inputs_tfidf[round(0.8 * inputs_tfidf.shape[0]):, :]\n",
    "y_test = targets[round(0.8 * targets.shape[0]):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]\n",
    "assert y_train.shape[1] == len(genres_dict.keys())\n",
    "assert y_test.shape[1] == len(genres_dict.keys())\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models folder\n",
    "models_path = './dltutorial/models/'\n",
    "\n",
    "if not os.path.exists(models_path):\n",
    "    print(\"Created %s folder...\" % models_path)\n",
    "    logging.debug(\"Created %s folder...\" % models_path)\n",
    "    os.makedirs(models_path)\n",
    "    \n",
    "else:\n",
    "    print(\"%s folder already exists...\" % models_path)\n",
    "    logging.debug(\"%s folder already exists...\" % models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_clf_file = './dltutorial/models/SVC.pckl'\n",
    "\n",
    "if not os.path.exists(SVM_clf_file):\n",
    "    logging.debug(\"Importing relevant scikit-learn packages...\")\n",
    "    # Scikit-learn imports\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import make_scorer\n",
    "    \n",
    "    # Parameters grid\n",
    "    param_grid = {'kernel':['linear'], 'C':[0.01, 0.1, 1.0]}\n",
    "    # Grid-Search Cross Validation\n",
    "    logging.debug(\"Grid-Search CV instantiation...\")\n",
    "    gridCV = GridSearchCV(SVC(class_weight='balanced'),\n",
    "                          param_grid=param_grid,\n",
    "                          scoring=make_scorer(f1_score, average='micro'))\n",
    "    # Multi-label/class strategy\n",
    "    logging.debug(\"Multi-label strategy classifier...\")\n",
    "    clf = OneVsRestClassifier(gridCV)\n",
    "    # Fitting the classifier\n",
    "    logging.debug(\"Fitting the classifier...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Store the model\n",
    "    logging.debug(\"Storing the classifier under %s...\" %\n",
    "                 SVM_clf_file)\n",
    "    with open(SVM_clf_file, 'wb') as f:\n",
    "        pickle.dump(clf, f)\n",
    "    logging.debug(\"Done writing the classifier under %s...\" %\n",
    "                 SVM_clf_file)\n",
    "    \n",
    "else:\n",
    "    logging.debug(\"Loading the classifier from %s...\" %\n",
    "                 SVM_clf_file)\n",
    "    with open(SVM_clf_file, 'rb') as f:\n",
    "        clf = pickle.load(f)\n",
    "    logging.debug(\"Done loading the classifier from %s...\" %\n",
    "                 SVM_clf_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant scikit-learn package\n",
    "from sklearn.metrics import classification_report\n",
    "# Predicting on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "# Printing results\n",
    "sorted_genre_names = [genres_dict[genre_id] for genre_id in sorted(genres_dict.keys())]\n",
    "print(classification_report(y_test, y_pred, target_names=sorted_genre_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use list comprehension\n",
    "predictions = list()\n",
    "for i in range(y_pred.shape[0]):\n",
    "    genres_pred = list()\n",
    "    movie_scores = y_pred[i]\n",
    "    for j in range(y_pred.shape[1]):\n",
    "        if movie_scores[j] > 0:\n",
    "            genres_pred.append(sorted_genre_names[j])\n",
    "    predictions.append(sorted(genres_pred))\n",
    "    \n",
    "for i in range(1, y_pred.shape[0], 50):\n",
    "    print(\"MOVIE:\\n\", overview_movies[i]['title'], '\\nPREDICTION:\\n',\n",
    "         ', '.join(predictions[i]), '\\nGROUND TRUTH:\\n', \n",
    "         ', '.join(sorted(genres_dict[genre_id] for genre_id in overview_movies[i]['genre_ids'])), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_clf_file = './dltutorial/models/NB.pckl'\n",
    "\n",
    "if not os.path.exists(NB_clf_file):\n",
    "    logging.debug(\"Importing relevant Naive Bayes modules from sklearn...\")\n",
    "    # Importing Multinomial Naive Bayes\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import make_scorer\n",
    "    # Parameters grid\n",
    "    param_grid = {'alpha': [10 ** i for i in range(-5, 1, 1)]}\n",
    "    # Grid-Search Cross Validation\n",
    "    logging.debug(\"Grid-Search CV instantiation...\")\n",
    "    gridCV = GridSearchCV(MultinomialNB(),\n",
    "                          param_grid=param_grid,\n",
    "                          scoring=make_scorer(f1_score, average='micro'))\n",
    "    # Classifier instantiation\n",
    "    logging.debug(\"Classifier instantiation...\")\n",
    "    clf_nb = OneVsRestClassifier(gridCV)\n",
    "    # Fitting the classifier\n",
    "    logging.debug(\"Fitting the algorithm on %s samples...\" %\n",
    "                 X_train.shape[0])\n",
    "    clf_nb.fit(X_train, y_train)\n",
    "    # Saving the fitted classifier in pickle format\n",
    "    logging.debug(\"Saving the fitted classifier on hard disk (%s)...\" %\n",
    "                 NB_clf_file)\n",
    "    with open(NB_clf_file, 'wb') as f:\n",
    "        pickle.dump(clf_nb, f)\n",
    "    logging.debug(\"Done saving the fitted Naive Bayes classifier...\")\n",
    "    \n",
    "else:\n",
    "    logging.debug(\"Loading the fitted Naive Bayes classifier from %s...\" %\n",
    "                 NB_clf_file)\n",
    "    with open(NB_clf_file, 'rb') as f:\n",
    "        clf_nb = pickle.load(f)\n",
    "    logging.debug(\"Done loading the fitted Naive Bayes classifier...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on test set\n",
    "y_pred_nb = clf_nb.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_nb, target_names=sorted_genre_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comments*\n",
    "\n",
    "Based on sklearn.utils.multiclass.type_of_target it makes sense\n",
    "that ``clf_nb.label_binarizer_.y_type_`` is ``'multilabel-indicator'`` as \n",
    "* 'multilabel-indicator': `y` is a label indicator matrix, an array\n",
    "of two dimensions with at least two columns, and at most 2 unique\n",
    "values.\n",
    "\n",
    "Is the validation pipeline wrong?\n",
    "\n",
    "TODO: Refactor using scikit-learn Pipeline.\n",
    "\n",
    "The hyper-parameter ``alpha`` might be playing an important role here. We build a ``GridSearchCV`` to investigate its impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use list comprehension\n",
    "predictions_nb = list()\n",
    "for i in range(y_pred_nb.shape[0]):\n",
    "    genres_pred = list()\n",
    "    movie_scores = y_pred_nb[i]\n",
    "    for j in range(y_pred_nb.shape[1]):\n",
    "        if movie_scores[j] > 0:\n",
    "            genres_pred.append(sorted_genre_names[j])\n",
    "    predictions_nb.append(sorted(genres_pred))\n",
    "predictions_nb[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, y_pred_nb.shape[0], 50):\n",
    "    print(\"MOVIE:\\n\", overview_movies[i]['title'], '\\nPREDICTION:\\n',\n",
    "         ', '.join(predictions_nb[i]), '\\nGROUND TRUTH:\\n',\n",
    "         ', '.join(sorted(genres_dict[genre_id] for genre_id in overview_movies[i]['genre_ids'])), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most movies do not get a prediction as the individual estimators are unable to beat the 0.5 classification threshold against all the other classes.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "* Multinomial Naive Bayes outperforms SVM on precision but has awful results in recall.\n",
    "* As a result SVM is better than Naive Bayes overall based on F1-score (defined as: $\\text{F1} = \\frac{2}{\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}}$).\n",
    "* We are unable to recreate Spandan Madan's results on Multinomial Naive Bayes even though this technique is recommended when dealing with text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deep Learning is about learning a space transformation that fits our classification needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting genre from poster image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We do not train the Convolutional Neural Network from scratch as it requires high computational power and a heavy dataset. Instead we use the ConvNet as an initialization or a fixed feature extractor for the task of interest.\n",
    "\n",
    "* We need to tackle 2 steps to build our Deep Learning classifier on poster images. First the feature engineering is done by using the pre-trained and released ConvNet as a feature extractor. It will transforms the data so that the posters that look similar will be closer together. Then we build a simple Neural Network on top which only learns to classify this dataset using the features obtained in step 1 as descriptors and the genre as labels.\n",
    "\n",
    "* Here we use a VGG-net (from Oxford University Visual Geometry Group) to extract the \"VGG features of an image\". We will use ``keras`` framework to chop off VGG-net last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model_path = './dltutorial/models/vgg16_model.h5'\n",
    "\n",
    "if not os.path.exists(vgg_model_path):\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    from keras.preprocessing import image as keras_image\n",
    "    from keras.applications.vgg16 import preprocess_input\n",
    "    vgg_model = VGG16(weights='imagenet', include_top=False)\n",
    "    logging.debug(\"Saving VGG16 model in %s...\" % vgg_model_path)\n",
    "    vgg_model.save(vgg_model_path)\n",
    "    logging.debug(\"Done saving VGG16 model in %s...\" % vgg_model_path)\n",
    "    \n",
    "else:\n",
    "    logging.debug(\"Loading VGG16 model from %s...\" % vgg_model_path)\n",
    "    from keras.models import load_model\n",
    "    vgg_model = load_model(vgg_model_path)\n",
    "    logging.debug(\"Done loading VGG16 model from %s...\" % vgg_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_extraction_file = './dltutorial/data/features_extraction.pckl'\n",
    "\n",
    "if not os.path.exists(features_extraction_file):\n",
    "    features_list = list()\n",
    "    labels_list = list()\n",
    "    movies_order_list = list()\n",
    "    failed_files_list = list()\n",
    "\n",
    "    image_names = [image for image in os.listdir(posters_path) if image.endswith('.jpg')]\n",
    "    logging.debug(\"%d image files found...\" % len(image_names))\n",
    "    for i, movie in enumerate(poster_movies):\n",
    "        poster_name = movie['original_title'].replace(' ', '_') + '.jpg'\n",
    "        if poster_name in image_names:\n",
    "            image_path = posters_path + poster_name\n",
    "            try:\n",
    "                image = keras_image.load_img(image_path, target_size=(224, 224))\n",
    "                x = keras_image.img_to_array(image)\n",
    "                x = np.expand_dims(x, axis=0)\n",
    "                x = preprocess_input(x)\n",
    "                features = vgg_model.predict(x)\n",
    "                if np.max(np.asarray(features)) == 0.0:\n",
    "                    logging.debug(\"Problematic movie (%s, %s)\" % (image_path, i))\n",
    "                    print(\"Problematic movie (%s, %s)\" % (image_path, i))\n",
    "                else:\n",
    "                    features_list.append(vgg_model.predict(x))\n",
    "                    movies_order_list.append(image_path)\n",
    "                    labels_list.append(movie['genre_ids'])\n",
    "                if i % (round(len(image_names) / 10)) == 0 and i != 0:\n",
    "                    logging.debug(\"Handling %d-th image...\" % i)\n",
    "                    print(\"Handling %d-th image...\" % i)\n",
    "            except:\n",
    "                logging.debug(\"Error handling movie %s (%d-th)\" % (movie, i))\n",
    "                failed_files_list.append(image_path)\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            logging.debug(\"Movie %s (%d-th) not in poster movies...\" % (movie, i))\n",
    "            continue\n",
    "    logging.debug(\"Done extracting features using VGG16 from %d movies...\" % len(movies_order_list))\n",
    "    print(\"Done extracting features using VGG16 from %d movies...\" % len(movies_order_list))\n",
    "    assert len(labels_list) == len(features_list)\n",
    "    assert len(labels_list) == len(movies_order_list)\n",
    "\n",
    "    # Saving features extraction in pickle format\n",
    "    features_extraction = (features_list, labels_list, movies_order_list, failed_files_list)\n",
    "    print(\"Saving features extraction in pickle format...\")\n",
    "    logging.debug(\"Saving features extraction tuple in pickle format on disk (%s)...\" %\n",
    "                 features_extraction_file)\n",
    "    with open(features_extraction_file, 'wb') as f:\n",
    "        pickle.dump(features_extraction, f)\n",
    "    logging.debug(\"Done saving features extraction tuple in pickle format...\")\n",
    "    print(\"Done saving...\")\n",
    "    \n",
    "else:\n",
    "    logging.debug(\"Loading features extraction tuple from hard disk (%s)...\" %\n",
    "                 features_extraction_file)\n",
    "    with open(features_extraction_file, 'rb') as f:\n",
    "        features_extraction = pickle.load(f)\n",
    "    logging.debug(\"Done loading features extraction tuple from hard disk...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve features list\n",
    "features_list = features_extraction[0]\n",
    "# Compute feature size\n",
    "feature_size = 1\n",
    "for dim in features_list[0].shape:\n",
    "    feature_size *= dim\n",
    "logging.debug(\"Feature size: %d\" % feature_size)\n",
    "# Building dataset\n",
    "X = np.zeros((len(features_list), feature_size))\n",
    "for i in range(len(features_list)):\n",
    "    X[i] = features_list[i].reshape(1, -1)\n",
    "logging.debug(\"X shape: (%d, %d)...\" % X.shape)\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "Y = MultiLabelBinarizer().fit_transform(features_extraction[1])\n",
    "logging.debug(\"Y shape: (%d, %d)...\" % Y.shape)\n",
    "assert X.shape[0] == Y.shape[0]\n",
    "assert X.shape[1] == feature_size\n",
    "assert Y.shape[1] == len(genres_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random seed\n",
    "np.random.seed(seed=42)\n",
    "# Mask array\n",
    "mask = np.random.rand(len(X)) < 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[mask]\n",
    "y_train = Y[mask]\n",
    "X_test = X[~mask]\n",
    "y_test = Y[~mask]\n",
    "\n",
    "assert X_train.shape[0] > X_test.shape[0]\n",
    "assert X_train.shape[1] == X_test.shape[1]\n",
    "assert y_train.shape[0] > y_test.shape[0]\n",
    "assert y_train.shape[1] == y_test.shape[1]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our shallow Neural Net using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poster_model_file = './dltutorial/models/poster_model.h5'\n",
    "\n",
    "if not os.path.exists(poster_model_file):\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation\n",
    "    from keras import optimizers\n",
    "    poster_model = Sequential([\n",
    "        Dense(1024, input_shape=(feature_size,)),\n",
    "        Activation('relu'),\n",
    "        Dense(256),\n",
    "        Activation('relu'),\n",
    "        Dense(y_train.shape[1]),\n",
    "        Activation('sigmoid')\n",
    "    ])\n",
    "\n",
    "    poster_model.compile(optimizer=optimizers.rmsprop(lr=1e-4, decay=1e-6),\n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    poster_model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "    \n",
    "    logging.debug(\"Saving poster model on hard disk (%s)...\" %\n",
    "                 poster_model_file)\n",
    "    poster_model.save(poster_model_file)\n",
    "    logging.debug(\"Done saving poster model on hard disk (%s)...\" %\n",
    "                 poster_model_file)\n",
    "    \n",
    "else:\n",
    "    logging.debug(\"Loading poster model from hard disk (%s)...\" %\n",
    "                 poster_model_file)\n",
    "    from keras.models import load_model\n",
    "    poster_model = load_model(poster_model_file)\n",
    "    logging.debug(\"Done loading poster model from hard disk (%s)...\" %\n",
    "                 poster_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_pred = poster_model.predict(X_test)\n",
    "logging.debug(\"predictions shape: %d, %d\" % y_pred.shape)\n",
    "assert y_pred.shape == y_test.shape\n",
    "# Convert continuous-multioutput into multilabel-indicator\n",
    "y_pred_indicators = np.zeros_like(y_pred, dtype=int)\n",
    "for i, pred in enumerate(y_pred):\n",
    "    positions = np.argsort(pred)[-3:]\n",
    "    for pos in positions:\n",
    "        y_pred_indicators[i][pos] = 1\n",
    "print(classification_report(y_test, y_pred_indicators, target_names=sorted_genre_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results interpretation:\n",
    "\n",
    "* **Disclaimer:** the following comparison between conventional and DL methods has little sense for several reasons:\n",
    "  * First the training and testing datasets are not quite the same size in the two methods (315 testing samples in conventional methods vs. 247 in DL).\n",
    "  * Secondly the classification problems are different in the sense that the conventional methods focus on text classification when Deep Learning is used on movie posters.\n",
    "  \n",
    "* That being said, we see that our Neural Net performs only slightly worse than the SVM classifier based on ``F1-score``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning to retrieve Textual Features\n",
    "\n",
    "In the next section we will be using Deep Learning to retrieve textual features and therefore compare apples to apples.\n",
    "\n",
    "Just as we did by using VGGnet for image classification, we will be using a Word2Vec model to get a meaningful word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_text_dl_file = './dltutorial/data/inputs_text_dl.pckl'\n",
    "targets_text_dl_file = './dltutorial/data/targets_text_dl.pckl'\n",
    "\n",
    "\n",
    "if not os.path.exists(inputs_text_dl_file) or not os.path.exists(targets_text_dl_file):\n",
    "    from gensim.models import KeyedVectors\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk import download\n",
    "    w2v_model_file = './dltutorial/models/GoogleNews-vectors-negative300.bin'\n",
    "    w2v_model = KeyedVectors.load_word2vec_format(w2v_model_file, binary=True, limit=500000)\n",
    "    \n",
    "    overview_movies_file = './dltutorial/data/overview_movies.pckl'\n",
    "\n",
    "    # Overview movies\n",
    "    logging.debug(\"Loading overview movies from disk (%s)...\" % overview_movies_file)\n",
    "    with open(overview_movies_file, 'rb') as f:\n",
    "        overview_movies = pickle.load(f)\n",
    "    logging.debug(\"Done loading %s overview movies...\" % len(overview_movies))\n",
    "    \n",
    "    \n",
    "    n_features = w2v_model['man'].shape[0]\n",
    "    logging.debug(\"Retrieving feature number: %d...\" % n_features)\n",
    "    n_samples = len(overview_movies)\n",
    "    logging.debug(\"Retrieving samples number: %d...\" % n_samples)\n",
    "    movies_mean_wordvec = np.zeros((n_samples, n_features))\n",
    "    logging.debug(\"Movie Word2Vec shape: (%d, %d)\" % movies_mean_wordvec.shape)\n",
    "    \n",
    "    ### Text preprocessing ###\n",
    "    \n",
    "    # Create regular expression tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # Downloading stopwords\n",
    "    download('stopwords')\n",
    "    # Create English stop words list\n",
    "    eng_stop_words = stopwords.words('english')\n",
    "    logging.debug(\"Retrieving English stop words...  (%d)\" % len(eng_stop_words))\n",
    "    \n",
    "    mask = list()\n",
    "    labels_list = list()\n",
    "    for i, movie in enumerate(overview_movies):\n",
    "        labels_list.append(movie['genre_ids'])\n",
    "        tokens = tokenizer.tokenize(movie['overview'])\n",
    "        filtered_tokens = [token.lower() for token in tokens if not token in eng_stop_words]\n",
    "        if len(filtered_tokens) == 0:\n",
    "            mask.append(False)\n",
    "            labels_list.pop(-1)\n",
    "        else:\n",
    "            vocab_counts = 0\n",
    "            movie_sum = 0\n",
    "            for token in filtered_tokens:\n",
    "                if token in w2v_model.vocab:\n",
    "                    vocab_counts += 1\n",
    "                    movie_sum += w2v_model[token]\n",
    "            if vocab_counts != 0:\n",
    "                movies_mean_wordvec[i] = movie_sum / float(vocab_counts)\n",
    "                mask.append(True)\n",
    "            else:\n",
    "                mask.append(False)\n",
    "                labels_list.pop(-1)\n",
    "                \n",
    "    assert len(mask) == len(labels_list)\n",
    "    print(\"Overview movies: %d...\" % len(mask))\n",
    "    print(\"Number of movies to be removed: %d...\" \n",
    "          % (len(movies_mean_wordvec) - len(mask)))\n",
    "\n",
    "    # Transform into numpy arrays to train on\n",
    "    from sklearn.preprocessing import MultiLabelBinarizer\n",
    "    X = movies_mean_wordvec[mask]\n",
    "    y = MultiLabelBinarizer().fit_transform(labels_list)\n",
    "    logging.debug(\"X shape... (%d, %d)\" % X.shape)\n",
    "    logging.debug(\"y shape... (%d, %d)\" % y.shape)\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    print(\"X shape... (%d, %d)\" % X.shape)\n",
    "    print(\"y shape... (%d, %d)\" % y.shape)\n",
    "    \n",
    "    # Saving into pickle\n",
    "    logging.debug(\"Saving inputs text (DL) on hard disk (%s)...\"\n",
    "                 % inputs_text_dl_file)\n",
    "    with open(inputs_text_dl_file, 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "    logging.debug(\"Done saving inputs text (DL) on hard disk...\")\n",
    "\n",
    "    logging.debug(\"Saving targets text (DL) on hard disk (%s)...\" \n",
    "                 % targets_text_dl_file)\n",
    "    with open(targets_text_dl_file, 'wb') as f:\n",
    "        pickle.dump(y, f)\n",
    "    logging.debug(\"Done saving targets text (DL) on hard disk...\")  \n",
    "    \n",
    "else:\n",
    "    logging.debug(\"Loading inputs text (DL) from hard disk (%s)...\"\n",
    "                 % inputs_text_dl_file)\n",
    "    with open(inputs_text_dl_file, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    logging.debug(\"Done loading inputs text (DL) from hard disk...\")\n",
    "    \n",
    "    logging.debug(\"Loading inputs text (DL) from hard disk (%s)...\"\n",
    "                 % targets_text_dl_file)\n",
    "    with open(targets_text_dl_file, 'rb') as f:\n",
    "        y = pickle.load(f)\n",
    "    logging.debug(\"Done loading targets text (DL) from hard disk...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random seed\n",
    "np.random.seed(seed=42)\n",
    "# Mask array\n",
    "mask = np.random.rand(len(X)) < 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1251, 300)\n",
      "y_train shape: (1251, 19)\n",
      "X_test shape: (326, 300)\n",
      "y_test shape: (326, 19)\n"
     ]
    }
   ],
   "source": [
    "X_train = X[mask]\n",
    "y_train = y[mask]\n",
    "X_test = X[~mask]\n",
    "y_test = y[~mask]\n",
    "\n",
    "assert X_train.shape[0] > X_test.shape[0]\n",
    "assert X_train.shape[1] == X_test.shape[1]\n",
    "assert y_train.shape[0] > y_test.shape[0]\n",
    "assert y_train.shape[1] == y_test.shape[1]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_model_file = './dltutorial/models/overview_model.h5'\n",
    "\n",
    "if not os.path.exists(overview_model_file):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation\n",
    "    from keras import optimizers\n",
    "    \n",
    "    # Build the Neural Network architecture\n",
    "    overview_model = Sequential([\n",
    "        Dense(X_train.shape[1], input_shape=(X_train.shape[1],)),\n",
    "        Activation('relu'),\n",
    "        Dense(y_train.shape[1]),\n",
    "        Activation('softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the Neural Network\n",
    "    overview_model.compile(optimizer=optimizers.rmsprop(lr=3e-4, decay=1e-6),\n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "    \n",
    "    # Train the neural network\n",
    "    overview_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Save the model\n",
    "    logging.debug(\"Saving the overview NN model on hard disk (%s)...\" %\n",
    "                 overview_model_file)\n",
    "    overview_model.save(overview_model_file)\n",
    "    logging.debug(\"Done saving the overview NN model on hard disk...\")\n",
    "    \n",
    "else:\n",
    "    # Load the model\n",
    "    logging.debug(\"Loading overview NN model from hard disk (%s)...\" %\n",
    "                 overview_model_file)\n",
    "    from keras.models import load_model\n",
    "    overview_model = load_model(overview_model_file)\n",
    "    logging.debug(\"Done loading overview NN model from hard disk...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "         Action       0.00      0.00      0.00        74\n",
      "      Adventure       0.00      0.00      0.00        59\n",
      "      Animation       0.00      0.00      0.00        63\n",
      "         Comedy       0.33      1.00      0.49       107\n",
      "          Crime       0.00      0.00      0.00        53\n",
      "    Documentary       0.00      0.00      0.00       101\n",
      "          Drama       0.22      1.00      0.36        72\n",
      "         Family       0.00      0.00      0.00        28\n",
      "        Fantasy       0.00      0.00      0.00         5\n",
      "        History       0.33      1.00      0.49       106\n",
      "         Horror       0.00      0.00      0.00        41\n",
      "          Music       0.00      0.00      0.00        29\n",
      "        Mystery       0.00      0.00      0.00        60\n",
      "        Romance       0.00      0.00      0.00        36\n",
      "Science Fiction       0.00      0.00      0.00        27\n",
      "       TV Movie       0.00      0.00      0.00        43\n",
      "       Thriller       0.00      0.00      0.00        58\n",
      "            War       0.00      0.00      0.00        20\n",
      "        Western       0.00      0.00      0.00         3\n",
      "\n",
      "    avg / total       0.09      0.29      0.13       985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inzouzouwetrust/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Loading genres dictionary\n",
    "genres_dict_file = './dltutorial/data/genres_dict.pckl'\n",
    "logging.debug(\"Loading movie genres dictionary from hard disk (%s)...\" \n",
    "             % genres_dict_file)\n",
    "with open(genres_dict_file, 'rb') as f:\n",
    "    genres_dict = pickle.load(f)\n",
    "logging.debug(\"Done loading movie genres dictionary from hard disk...\")\n",
    "assert len(genres_dict.keys()) == y_test.shape[1]\n",
    "sorted_genre_names = sorted([genre for genre in genres_dict.values()])\n",
    "\n",
    "# Prediction\n",
    "y_pred = overview_model.predict(X_test)\n",
    "logging.debug(\"predictions shape: %d, %d\" % y_pred.shape)\n",
    "assert y_pred.shape == y_test.shape\n",
    "\n",
    "# Convert continuous-multioutput into multilabel-indicator\n",
    "y_pred_indicators = np.zeros_like(y_pred, dtype=int)\n",
    "for i, pred in enumerate(y_pred):\n",
    "    # Take the first 3 movies\n",
    "    positions = np.argsort(pred)[-3:]\n",
    "    for pos in positions:\n",
    "        y_pred_indicators[i][pos] = 1\n",
    "        \n",
    "print(classification_report(y_test, y_pred_indicators, target_names=sorted_genre_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We see clear overfitting on Comedy, Documentary and History - the 3 most representated movie genres in our testing set.\n",
    "\n",
    "* The trend does not improve if we include more genres per prediction. In fact it seems the model has overfitted on the most represented genres (regardless of the training dataset??). Including the top 6 genres improves the both the ``recall`` and ``f1-score`` but is not realistic."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
